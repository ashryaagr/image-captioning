{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6vILc2k0ZcF0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import LongTensor\n",
    "from torch.nn import Embedding, LSTM\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "L2Z1l7ovZzD0"
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "#credit: https://gist.githubusercontent.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec/raw/60dc6be30ba57aa5d0d036e6af8ff702782ded18/pad_packed_demo.py\n",
    "## We want to run LSTM on a batch of 3 character sequences ['long_str', 'tiny', 'medium']\n",
    "#\n",
    "#     Step 1: Construct Vocabulary\n",
    "#     Step 2: Load indexed data (list of instances, where each instance is list of word indices)\n",
    "#     Step 3: Make Model\n",
    "#  *  Step 4: Pad instances with 0s till max length sequence\n",
    "#  *  Step 5: Sort instances by sequence length in descending order\n",
    "#  *  Step 6: Embed the instances\n",
    "#  *  Step 7: Call pack_padded_sequence with embeded instances and sequence lengths\n",
    "#  *  Step 8: Forward with LSTM\n",
    "#  *  Step 9: Call unpack_padded_sequences if required / or just pick last hidden vector\n",
    "#  *  Summary of Shape Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LIzKod1iZ521"
   },
   "outputs": [],
   "source": [
    "# We want to run LSTM on a batch following 3 character sequences\n",
    "seqs = ['a small sentence',  # len = 3\n",
    "        'little bit bigger sentence',      # len = 4\n",
    "        'nothing to say']    # len = 3\n",
    "test = 'new word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dBj_Q7hxaFSc"
   },
   "outputs": [],
   "source": [
    "## Step 1: Construct Vocabulary ##\n",
    "##------------------------------##\n",
    "# make sure <pad> idx is 0\n",
    "vocab = ['<pad>', '<start>', '<end>', '<unk>'] + sorted(set([word for seq in seqs for word in seq.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "1YBFmR7OaU-D",
    "outputId": "06eccf6c-a02b-45fe-c25e-e0adaf79cdbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " '<start>',\n",
       " '<end>',\n",
       " '<unk>',\n",
       " 'a',\n",
       " 'bigger',\n",
       " 'bit',\n",
       " 'little',\n",
       " 'nothing',\n",
       " 'say',\n",
       " 'sentence',\n",
       " 'small',\n",
       " 'to']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ed2zeu2saRJ-",
    "outputId": "7d3503ca-8656-4510-f7ff-311ebc870e5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 11, 10], [7, 6, 5, 10], [8, 12, 9]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 2: Load indexed data (list of instances, where each instance is list of character indices) ##\n",
    "##-------------------------------------------------------------------------------------------------##\n",
    "vectorized_seqs = [[vocab.index(tok) for tok in seq.split()]for seq in seqs]\n",
    "vectorized_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "96m133RbaduM"
   },
   "outputs": [],
   "source": [
    "## Step 3: Make Model ##\n",
    "##--------------------##\n",
    "embed = Embedding(len(vocab), 4) # embedding_dim = 4\n",
    "lstm = LSTM(input_size=4, hidden_size=5, num_layers = 1, batch_first=True) # input_dim = 4, hidden_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YHFPT15DakaG"
   },
   "outputs": [],
   "source": [
    "## Step 4: Pad instances with 0s till max length sequence ##\n",
    "##--------------------------------------------------------##\n",
    "\n",
    "# get the length of each seq in your batch\n",
    "seq_lengths = LongTensor(list(map(len, vectorized_seqs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cIj1h-ZXg3kN",
    "outputId": "e8c0593e-ee68-4eaf-b711-9c780ebe9a4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Dz9rjmqeg2W2"
   },
   "outputs": [],
   "source": [
    "# seq_lengths => [3, 4, 3]\n",
    "# batch_sum_seq_len: 3 + 4 + 3 = 10\n",
    "# max_seq_len: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "F8hGYQQthDd9",
    "outputId": "5ded70a6-69e3-4e25-df36-c2bfc7706b3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "PfdIPYcKhIfC"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (4) must match the existing size (3) at non-singleton dimension 0.  Target sizes: [4].  Tensor sizes: [3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2303/3249600610.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mseq_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseqlen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (4) must match the existing size (3) at non-singleton dimension 0.  Target sizes: [4].  Tensor sizes: [3]"
     ]
    }
   ],
   "source": [
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = LongTensor(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "Mm82r_qthMCx",
    "outputId": "a3eeea1f-a019-443c-c534-24cb67c72197"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "9Ms1QmuGhqh-",
    "outputId": "7665d079-7f40-4276-bf84-8b9a6907d274"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 5: Sort instances by sequence length in descending order ##\n",
    "##---------------------------------------------------------------##\n",
    "\n",
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "-wk2x_d8hwIA",
    "outputId": "53032583-f19b-444b-ba7e-d66a6c4ee445"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "         [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "         [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "         [ 0.9113,  1.0259, -1.4606, -1.1177]],\n",
       "\n",
       "        [[ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "         [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "         [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "         [ 0.9113,  1.0259, -1.4606, -1.1177]],\n",
       "\n",
       "        [[ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "         [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "         [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "         [ 0.9113,  1.0259, -1.4606, -1.1177]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 6: Embed the instances ##\n",
    "##-----------------------------##\n",
    "\n",
    "embedded_seq_tensor = embed(seq_tensor)\n",
    "embedded_seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Wst8PO0_m4KW",
    "outputId": "3f8bef07-f26c-46d9-ebc7-88cd8769da38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_seq_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "AnOJumDBh4lE",
    "outputId": "a22f53bd-4a0b-41e1-af50-96da81c99f02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "        [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "        [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "        [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "        [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "        [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "        [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "        [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "        [ 0.9113,  1.0259, -1.4606, -1.1177],\n",
       "        [ 0.9113,  1.0259, -1.4606, -1.1177]],\n",
       "       grad_fn=<PackPaddedSequenceBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 7: Call pack_padded_sequence with embeded instances and sequence lengths ##\n",
    "##-------------------------------------------------------------------------------##\n",
    "\n",
    "packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "# packed_input (PackedSequence is NamedTuple with 2 attributes: data and batch_sizes\n",
    "packed_input.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "4M7Q8d4kiEfd",
    "outputId": "74eed30e-8e9a-4855-9911-2bd04b3354f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_input.data.shape #(batch_wise_sum_seq_len X embedding_dim) = (10 X 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6TQGYlSYnHFI",
    "outputId": "fd051b13-e4ce-4c89-a471-0fca2565db52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_input.batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "QG9_OlwynL6R"
   },
   "outputs": [],
   "source": [
    "# visualization :\n",
    "# little  bit    longer    sentence\n",
    "# a       small  sentence \n",
    "# nothing to     say\n",
    "# 3  3  3  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "gdzNGPLHiAG0"
   },
   "outputs": [],
   "source": [
    "# tensor([[-0.3227, -0.1044, -0.4612, -0.8055], #little\n",
    "#         [ 0.6384,  0.5617,  0.6570,  1.0578], #a\n",
    "#         [-0.7129,  0.3673,  0.0192, -0.4796], #nothing\n",
    "\n",
    "#         [-0.6661, -1.5316,  0.6446, -1.3370], #bit\n",
    "#         [-0.2879,  2.3274,  0.8726,  1.0885], #small\n",
    "#         [-0.1367, -0.2717, -0.2533, -1.3797], #to\n",
    "\n",
    "#         [-0.4653, -0.4362,  0.7046, -0.8728], #bigger\n",
    "#         [-0.3567, -0.0277,  1.1684,  0.8097], #sentence\n",
    "#         [ 0.9794, -0.4929, -1.6183, -0.6653], #say\n",
    "\n",
    "#         [-0.3567, -0.0277,  1.1684,  0.8097]]) #sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "mR947cXgnuYj"
   },
   "outputs": [],
   "source": [
    "## Step 8: Forward with LSTM ##\n",
    "##---------------------------##\n",
    "\n",
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "# packed_output (PackedSequence is NamedTuple with 2 attributes: data and batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "KOG-3vuDZ_rG"
   },
   "outputs": [],
   "source": [
    "# ## Step 9: Call unpack_padded_sequences if required / or just pick last hidden vector ##\n",
    "# ##------------------------------------------------------------------------------------##\n",
    "\n",
    "# # unpack your output if required\n",
    "# output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "# # output.shape : ( batch_size X max_seq_len X hidden_dim) = (3 X 4 X 5)\n",
    "\n",
    "# # Or if you just want the final hidden state?\n",
    "# print(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "WcMmBpYUbiR7"
   },
   "outputs": [],
   "source": [
    "## Summary of Shape Transformations ##\n",
    "##----------------------------------##\n",
    "\n",
    "# (batch_size X max_seq_len X embedding_dim) --> Sort by seqlen ---> (batch_size X max_seq_len X embedding_dim)\n",
    "# (batch_size X max_seq_len X embedding_dim) --->      Pack     ---> (batch_sum_seq_len X embedding_dim)\n",
    "# (batch_sum_seq_len X embedding_dim)        --->      LSTM     ---> (batch_sum_seq_len X hidden_dim)\n",
    "# (batch_sum_seq_len X hidden_dim)           --->    UnPack     ---> (batch_size X max_seq_len X hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "9pgsZeKOoDcS"
   },
   "outputs": [],
   "source": [
    "################ calculate loss ##############\n",
    "# there are two ways to calculate losses\n",
    "# using CrossEntropyLoss() = logSoftmax + NLLLoss()\n",
    "# using NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "qDAgW99xoSCE"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "axUEjCiypK8S"
   },
   "outputs": [],
   "source": [
    "#lets assume for the sake of tutorial that targets = packed_input\n",
    "targets = seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "dPxf21QupTcN",
    "outputId": "58109e5f-9dbe-4b44-a203-fa09c72ee137"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "CyNEhuNuqKmR"
   },
   "outputs": [],
   "source": [
    "targets = pack_padded_sequence(targets, seq_lengths.cpu().numpy(), batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IA4ZsSloqak8",
    "outputId": "1c53bdc1-46c9-4a68-8000-75ddb4bea001"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), batch_sizes=tensor([3, 3, 3, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "0sDMajifp03_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(5, len(vocab)) #hidden_size, vocab_size\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "i8sopE0ep-ev"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0998,  0.1086,  0.1034, -0.0476, -0.0935],\n",
      "        [-0.0998,  0.1086,  0.1034, -0.0476, -0.0935],\n",
      "        [-0.0998,  0.1086,  0.1034, -0.0476, -0.0935],\n",
      "        [-0.1242,  0.1706,  0.1814, -0.0461, -0.1279],\n",
      "        [-0.1242,  0.1706,  0.1814, -0.0461, -0.1279],\n",
      "        [-0.1242,  0.1706,  0.1814, -0.0461, -0.1279],\n",
      "        [-0.1282,  0.2032,  0.2331, -0.0360, -0.1404],\n",
      "        [-0.1282,  0.2032,  0.2331, -0.0360, -0.1404],\n",
      "        [-0.1282,  0.2032,  0.2331, -0.0360, -0.1404],\n",
      "        [-0.1276,  0.2207,  0.2657, -0.0273, -0.1449]], grad_fn=<CatBackward>)\n",
      "tensor([[ 0.4589, -0.2719, -0.2125,  0.4503,  0.0342,  0.3797, -0.4292, -0.3648,\n",
      "          0.3602,  0.0482, -0.5181, -0.1793,  0.2977],\n",
      "        [ 0.4589, -0.2719, -0.2125,  0.4503,  0.0342,  0.3797, -0.4292, -0.3648,\n",
      "          0.3602,  0.0482, -0.5181, -0.1793,  0.2977],\n",
      "        [ 0.4589, -0.2719, -0.2125,  0.4503,  0.0342,  0.3797, -0.4292, -0.3648,\n",
      "          0.3602,  0.0482, -0.5181, -0.1793,  0.2977],\n",
      "        [ 0.4700, -0.2244, -0.1521,  0.4970, -0.0172,  0.4179, -0.4207, -0.4257,\n",
      "          0.3373,  0.1054, -0.5516, -0.2094,  0.3147],\n",
      "        [ 0.4700, -0.2244, -0.1521,  0.4970, -0.0172,  0.4179, -0.4207, -0.4257,\n",
      "          0.3373,  0.1054, -0.5516, -0.2094,  0.3147],\n",
      "        [ 0.4700, -0.2244, -0.1521,  0.4970, -0.0172,  0.4179, -0.4207, -0.4257,\n",
      "          0.3373,  0.1054, -0.5516, -0.2094,  0.3147],\n",
      "        [ 0.4722, -0.1989, -0.1221,  0.5262, -0.0452,  0.4454, -0.4149, -0.4617,\n",
      "          0.3259,  0.1398, -0.5611, -0.2334,  0.3339],\n",
      "        [ 0.4722, -0.1989, -0.1221,  0.5262, -0.0452,  0.4454, -0.4149, -0.4617,\n",
      "          0.3259,  0.1398, -0.5611, -0.2334,  0.3339],\n",
      "        [ 0.4722, -0.1989, -0.1221,  0.5262, -0.0452,  0.4454, -0.4149, -0.4617,\n",
      "          0.3259,  0.1398, -0.5611, -0.2334,  0.3339],\n",
      "        [ 0.4720, -0.1849, -0.1060,  0.5439, -0.0610,  0.4633, -0.4115, -0.4827,\n",
      "          0.3200,  0.1602, -0.5632, -0.2494,  0.3482]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "outputs = linear(packed_output.data)\n",
    "print(packed_output.data)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TQwHMX97qvxw",
    "outputId": "2cf4738f-0486-4cad-9ead-f98271752673"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 13])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7ojhyYLMo6di",
    "outputId": "3b88ede1-ea9b-4cde-d1f6-4fffb3ba885f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5446, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(outputs, targets.data) \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "H8aDT7sApKQG",
    "outputId": "a9dee345-761d-491f-c8bd-83e072857287"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5446, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_2 = nn.NLLLoss()\n",
    "loss = criterion(F.log_softmax(outputs, dim=1), targets.data) \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "SCy26IYqrLBK"
   },
   "outputs": [],
   "source": [
    "########### Generation #################\n",
    "# For generating, you will want to generate one word at a time, but for tutorial's sake we are reusing the outputs generated above \n",
    "# to dicuss the main difference between two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "DD1E1lr_rn4C"
   },
   "outputs": [],
   "source": [
    "# Deterministic: get the maximum output from output at each step of generation\n",
    "_, predicted = outputs.max(1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "nNX-85RMtixb",
    "outputId": "7b177351-4524-4096-e3fa-04e96edf74dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "ADGPTOjztBsk"
   },
   "outputs": [],
   "source": [
    "# Stochastic: sample from weighted softmax distribution\n",
    "temperature = 1\n",
    "probabilities = F.softmax(outputs.div(temperature).squeeze(0).squeeze(0), dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "nydZcTrEtWIT",
    "outputId": "872b699e-e14e-4ac5-df74-d984b1bdf362"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1145, 0.0551, 0.0585, 0.1135, 0.0749, 0.1057, 0.0471, 0.0502, 0.1037,\n",
       "         0.0759, 0.0431, 0.0605, 0.0974],\n",
       "        [0.1145, 0.0551, 0.0585, 0.1135, 0.0749, 0.1057, 0.0471, 0.0502, 0.1037,\n",
       "         0.0759, 0.0431, 0.0605, 0.0974],\n",
       "        [0.1145, 0.0551, 0.0585, 0.1135, 0.0749, 0.1057, 0.0471, 0.0502, 0.1037,\n",
       "         0.0759, 0.0431, 0.0605, 0.0974],\n",
       "        [0.1144, 0.0571, 0.0614, 0.1176, 0.0703, 0.1086, 0.0470, 0.0467, 0.1002,\n",
       "         0.0795, 0.0412, 0.0580, 0.0980],\n",
       "        [0.1144, 0.0571, 0.0614, 0.1176, 0.0703, 0.1086, 0.0470, 0.0467, 0.1002,\n",
       "         0.0795, 0.0412, 0.0580, 0.0980],\n",
       "        [0.1144, 0.0571, 0.0614, 0.1176, 0.0703, 0.1086, 0.0470, 0.0467, 0.1002,\n",
       "         0.0795, 0.0412, 0.0580, 0.0980],\n",
       "        [0.1137, 0.0581, 0.0628, 0.1200, 0.0678, 0.1107, 0.0468, 0.0447, 0.0982,\n",
       "         0.0815, 0.0405, 0.0562, 0.0990],\n",
       "        [0.1137, 0.0581, 0.0628, 0.1200, 0.0678, 0.1107, 0.0468, 0.0447, 0.0982,\n",
       "         0.0815, 0.0405, 0.0562, 0.0990],\n",
       "        [0.1137, 0.0581, 0.0628, 0.1200, 0.0678, 0.1107, 0.0468, 0.0447, 0.0982,\n",
       "         0.0815, 0.0405, 0.0562, 0.0990],\n",
       "        [0.1130, 0.0586, 0.0634, 0.1215, 0.0663, 0.1121, 0.0467, 0.0435, 0.0971,\n",
       "         0.0828, 0.0401, 0.0549, 0.0999]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "P66dfe2atNGY",
    "outputId": "73c3b83b-aa33-46fb-c810-012f1dfbb39b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = torch.multinomial(probabilities.data, 1)\n",
    "predicted\n",
    "\n",
    "predi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "MkcuXzOVteiK",
    "outputId": "3e4c0329-de26-447c-b48a-6af8159435ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 4],\n",
       "        [11],\n",
       "        [ 5],\n",
       "        [ 1],\n",
       "        [10],\n",
       "        [ 7],\n",
       "        [ 1]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = torch.multinomial(probabilities.data, 1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbZsG2buwGYB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
